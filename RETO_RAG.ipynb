{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ea4eedc",
      "metadata": {
        "id": "0ea4eedc"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0c07e509",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "0c07e509",
        "outputId": "a263cdf8-89e6-49a6-a4f5-4a6c50db20bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_chroma'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1422782673.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Vector store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_chroma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_chroma'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain core\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Vector store\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Loaders\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader\n",
        "\n",
        "# OpenAI\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n de la API Key de OpenAI\n",
        "import os\n",
        "\n",
        "# üîë Opci√≥n 1: Def√≠nela manualmente aqu√≠ (no recomendado en notebooks p√∫blicos)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key_aqui\"\n",
        "\n",
        "# üîë Opci√≥n 2 (recomendada): si est√°s en Colab, usa el gestor de secretos\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "except Exception as e:\n",
        "    # Si no est√°s en Colab, intenta leer de variable local\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(\"‚ùå No se encontr√≥ OPENAI_API_KEY. Def√≠nela antes de continuar.\")\n",
        "\n",
        "print(\"‚úÖ API Key configurada correctamente\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulgJAyE6k9br",
        "outputId": "f0802b4b-0970-4f41-d3e4-39dc576eebb4"
      },
      "id": "ulgJAyE6k9br",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API Key configurada correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e19bdb",
      "metadata": {
        "id": "22e19bdb"
      },
      "source": [
        "## 1. Extracting files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir directorios de trabajo\n",
        "\n",
        "# Carpeta donde estar√°n los documentos (definida en Celda 4 tambi√©n)\n",
        "DATA_DIR = Path(\"./data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Carpeta donde se guardar√° la base vectorial Chroma\n",
        "PERSIST_DIR = Path(\"./data/chroma\")\n",
        "PERSIST_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(\"‚úÖ Directorios listos\")\n",
        "print(\"üìÇ DATA_DIR:\", DATA_DIR.resolve())\n",
        "print(\"üìÇ PERSIST_DIR:\", PERSIST_DIR.resolve())"
      ],
      "metadata": {
        "id": "SOtbwefdlMur"
      },
      "id": "SOtbwefdlMur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Carpeta donde deben estar tus documentos (PDF, DOCX, TXT, MD, HTML)\n",
        "DATA_DIR = Path(\"./data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Entorno listo. Los documentos se tomar√°n desde:\", DATA_DIR.resolve())\n",
        "\n",
        "# Listar archivos encontrados en ./data\n",
        "print(\"üì¶ Archivos detectados en ./data:\")\n",
        "for p in DATA_DIR.glob(\"*\"):\n",
        "    if p.suffix.lower() in [\".pdf\", \".docx\", \".doc\", \".txt\", \".md\", \".html\", \".htm\"]:\n",
        "        print(\" -\", p.name)"
      ],
      "metadata": {
        "id": "VXTU4Su0lTNb"
      },
      "id": "VXTU4Su0lTNb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "folder_path = str(DATA_DIR)  # <-- usamos la carpeta definida en la celda 4\n",
        "\n",
        "# Buscar archivos comunes (recursivo)\n",
        "patterns = [\"**/*.pdf\", \"**/*.txt\", \"**/*.md\", \"**/*.docx\", \"**/*.html\", \"**/*.htm\"]\n",
        "file_paths = []\n",
        "for p in patterns:\n",
        "    file_paths.extend(glob(os.path.join(folder_path, p), recursive=True))\n",
        "\n",
        "if not file_paths:\n",
        "    raise FileNotFoundError(\n",
        "        f\"No se encontraron archivos en {folder_path}. \"\n",
        "        \"Coloca tus documentos en ./data.\"\n",
        "    )\n",
        "\n",
        "docs = []\n",
        "for path in file_paths:\n",
        "    try:\n",
        "        low = path.lower()\n",
        "        if low.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(path)\n",
        "        elif low.endswith(\".txt\") or low.endswith(\".md\"):\n",
        "            loader = TextLoader(path)\n",
        "        else:\n",
        "            loader = UnstructuredFileLoader(path)  # Para DOCX/HTML\n",
        "        docs.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error cargando {os.path.basename(path)}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Se cargaron {len(docs)} fragmentos desde {len(file_paths)} archivos\")"
      ],
      "metadata": {
        "id": "WYCOTmn_lqef"
      },
      "id": "WYCOTmn_lqef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6b83e77a",
      "metadata": {
        "id": "6b83e77a"
      },
      "source": [
        "## 2. Text Splitting into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b8f501",
      "metadata": {
        "id": "a4b8f501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce58aebd-2608-4fa1-f9a4-e447a01f57ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Se crearon 49 chunks\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Definir par√°metros de fragmentaci√≥n\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Dividir los documentos cargados en fragmentos\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"‚úÖ Se crearon {len(split_docs)} fragmentos (chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")\n",
        "\n",
        "# Mostrar vista previa del primer fragmento\n",
        "if split_docs:\n",
        "    print(\"Ejemplo de fragmento:\")\n",
        "    print(\"Archivo:\", split_docs[0].metadata.get(\"filename\", \"desconocido\"))\n",
        "    print(\"Texto:\", split_docs[0].page_content[:200].replace(\"\\n\", \" \"), \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d06e27f",
      "metadata": {
        "id": "8d06e27f"
      },
      "source": [
        "## 3. Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec00fa4b",
      "metadata": {
        "id": "ec00fa4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b371beb-9739-42dd-8059-eb07a303d85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Embeddings listos (dim=1536)\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Crear embeddings con el modelo moderno (m√°s r√°pido y econ√≥mico que ada-002)\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    api_key=openai_api_key   # aseg√∫rate que tu API Key est√° cargada en la celda 1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Embeddings listos con text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c82e20",
      "metadata": {
        "id": "72c82e20"
      },
      "source": [
        "## 4. Vector Stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d89880",
      "metadata": {
        "id": "a3d89880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfb3a73-aba8-4861-a64e-a12013c5d497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Vector store creado y guardado en: /content/chroma_db\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from pathlib import Path\n",
        "\n",
        "# Carpeta donde se guardar√° la base vectorial\n",
        "PERSIST_DIR = Path(\"./data/chroma\")\n",
        "PERSIST_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Crear y persistir el vector store con los fragmentos y los embeddings ya definidos\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=split_docs,      # viene de la celda 8\n",
        "    embedding=embeddings,      # viene de la celda 10\n",
        "    persist_directory=str(PERSIST_DIR)\n",
        ")\n",
        "\n",
        "vectordb.persist()\n",
        "print(f\"‚úÖ Vector store creado y guardado en: {PERSIST_DIR.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85da1ee7",
      "metadata": {
        "id": "85da1ee7"
      },
      "source": [
        "## 5. Retriving from the Persistant Vector Datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9fd2322",
      "metadata": {
        "id": "e9fd2322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f32dd91-3cca-44a3-beef-777413ca75ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Store reabierto y retriever listo.\n",
            "üìÅ Persist directory: /content/chroma_db\n",
            "üîé Par√°metros de b√∫squeda: {'k': 4}\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Reabrir Chroma desde disco usando los mismos embeddings\n",
        "vectordb = Chroma(\n",
        "    persist_directory=str(PERSIST_DIR),\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "# Preparar el retriever (k=4 por defecto)\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "print(\"‚úÖ Chroma reabierto desde:\", PERSIST_DIR.resolve())\n",
        "print(\"üîé Retriever listo (k=4)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4abf77e",
      "metadata": {
        "id": "d4abf77e"
      },
      "source": [
        "## 6. Retrivers in Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "568bce4f",
      "metadata": {
        "id": "568bce4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952de730-f7f5-4668-de57-974c3ee13868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Pregunta 1: ¬øQu√© informaci√≥n principal contienen estos documentos?\n",
            "üß† Respuesta:\n",
            " Los documentos contienen informaci√≥n sobre procedimientos internos relacionados con la recepci√≥n, diagn√≥stico y reparaci√≥n de equipos de c√≥mputo. Incluyen secciones como \"Inicio\", \"Nuevo Registro\", \"Editar Registro\", \"Cerrar Reparaci√≥n\", \"Imprimir Reporte\" y un anexo. Tambi√©n se menciona la revisi√≥n y emisi√≥n de un procedimiento espec√≠fico con un c√≥digo y la ubicaci√≥n de la documentaci√≥n en un m√≥dulo de control documental.\n",
            "\n",
            "üîé Pregunta 2: ¬øLa impresora atasca las hojas?\n",
            "üß† Respuesta:\n",
            " S√≠, una de las fallas comunes es que el papel se traba siempre. La soluci√≥n recomendada es llevar la impresora a mantenimiento al servicio t√©cnico.\n"
          ]
        }
      ],
      "source": [
        "# 6) Retrieval con LLM (QA) y m√∫ltiples consultas ‚Äî con citas y formato soporte\n",
        "# ----------------------------------------------------------------------------\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pathlib import Path\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=openai_api_key,  # ya definido en la celda 1\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Prompt en espa√±ol con formato de soporte\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=(\n",
        "        \"Eres un asistente de soporte t√©cnico. Responde en espa√±ol usando SOLO el CONTEXTO.\\n\"\n",
        "        \"Formato exacto:\\n\"\n",
        "        \"**Diagn√≥stico breve**\\n- ‚Ä¶\\n\\n\"\n",
        "        \"**Pasos para resolver**\\n1. ‚Ä¶\\n2. ‚Ä¶\\n3. ‚Ä¶\\n\\n\"\n",
        "        \"**Verificaci√≥n**\\n- ‚Ä¶\\n\\n\"\n",
        "        \"**Notas / Advertencias**\\n- ‚Ä¶\\n\\n\"\n",
        "        \"_Citas_: [archivo:p√°gina] [archivo:p√°gina]\\n\"\n",
        "        \"Si no hay evidencia suficiente en el CONTEXTO, dilo expl√≠citamente.\\n\\n\"\n",
        "        \"[CONTEXTO]\\n{context}\\n\\n[PREGUNTA]\\n{question}\\n\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Chain de QA con prompt y retorno de fuentes\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,                  # definido en celda 14\n",
        "    return_source_documents=True,         # <-- important√≠simo para citas\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": QA_PROMPT}\n",
        ")\n",
        "\n",
        "# Utilidad para renderizar citas\n",
        "def _cite(meta: dict) -> str:\n",
        "    page = meta.get(\"page\") or meta.get(\"page_number\") or meta.get(\"source_page\") or \"?\"\n",
        "    fn = Path(meta.get(\"filename\", meta.get(\"source\",\"doc\"))).name\n",
        "    return f\"[{fn}:{page}]\"\n",
        "\n",
        "# Lista de preguntas\n",
        "queries = [\n",
        "    \"¬øQu√© informaci√≥n principal contienen estos documentos?\",\n",
        "    \"¬øLa impresora atasca las hojas?\"\n",
        "]\n",
        "\n",
        "# Ejecutar todas las consultas\n",
        "for i, query in enumerate(queries, 1):\n",
        "    result = qa.invoke({\"query\": query})\n",
        "    answer = result.get(\"result\", \"\")\n",
        "    sources = result.get(\"source_documents\", []) or []\n",
        "\n",
        "    # Construir pie de citas si el modelo no las incluy√≥\n",
        "    if \"_Citas_:\" not in answer and sources:\n",
        "        cites = \" \".join(_cite(d.metadata) for d in sources[:3])\n",
        "        answer += f\"\\n\\n_Citas_: {cites}\"\n",
        "\n",
        "    print(f\"\\nüîé Pregunta {i}: {query}\\n\")\n",
        "    print(\"üß† Respuesta:\\n\", answer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag_build",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}